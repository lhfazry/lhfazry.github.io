<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-05-16T06:25:51+07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lhuqita Fazry</title><subtitle>personal description</subtitle><author><name>Lhuqita Fazry</name><email>lhuqita.fazry@ui.ac.id</email></author><entry><title type="html">Advancing Continual Learning: Tackling Catastrophic Forgetting and Loss of Plasticity with UPGD</title><link href="http://localhost:4000/posts/2025/05/13/advancing-continual-learning-tackling-catastrophic-forgetting-and-loss-of-plasticity-with-upgd/" rel="alternate" type="text/html" title="Advancing Continual Learning: Tackling Catastrophic Forgetting and Loss of Plasticity with UPGD" /><published>2025-05-13T00:00:00+07:00</published><updated>2025-05-13T00:00:00+07:00</updated><id>http://localhost:4000/posts/2025/05/13/advancing-continual-learning-tackling-catastrophic-forgetting-and-loss-of-plasticity-with-upgd</id><content type="html" xml:base="http://localhost:4000/posts/2025/05/13/advancing-continual-learning-tackling-catastrophic-forgetting-and-loss-of-plasticity-with-upgd/"><![CDATA[<p>In the realm of artificial intelligence (AI) and machine learning, <strong>continual learning</strong>—also known as lifelong learning—refers to the ability of a model to learn from a continuous stream of data over time, adapting to new information while retaining previously acquired knowledge. This contrasts with traditional machine learning paradigms, which often involve training models on static datasets, leading to a fixed understanding of the problem domain. Continual learning aims to emulate human-like learning processes, where individuals build upon past experiences and knowledge as they encounter new situations.</p>

<p>However, the pursuit of effective continual learning is fraught with challenges, particularly the phenomena known as <strong>catastrophic forgetting</strong> and <strong>loss of plasticity</strong>. Catastrophic forgetting occurs when a neural network, upon learning new tasks, inadvertently overwrites or forgets previously learned information. This issue is particularly pronounced in gradient-based learning methods, where the model’s weights are updated based on new data, potentially erasing valuable insights from earlier tasks. As a result, the model may exhibit diminished performance when faced with previously encountered tasks, rendering it less effective in dynamic environments where adaptability is paramount.</p>

<p>In tandem with catastrophic forgetting, loss of plasticity presents another significant hurdle. Plasticity refers to a model’s ability to adapt and learn from new information. As a neural network accumulates knowledge, its capacity to learn new tasks may diminish, leading to a slower and less effective learning process. This loss of plasticity can severely restrict a model’s performance, hampering its ability to respond to evolving data distributions and novel challenges.</p>

<p>Addressing these two intertwined issues—catastrophic forgetting and loss of plasticity—is crucial for the advancement of continual learning methodologies. The paper titled <strong>“Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning”</strong> introduces an innovative approach known as <strong>Utility-based Perturbed Gradient Descent (UPGD)</strong>. This method aims to simultaneously tackle both phenomena by modifying the learning process based on the utility of network units. By effectively mitigating these challenges, UPGD represents a significant step forward in the development of robust and adaptable AI systems capable of lifelong learning. This paper was published as a conference paper at International Conference on Learning Representations (ICLR) 2024. You can download the paper from <a href="https://arxiv.org/abs/2404.00781">here</a>.</p>

<h2 id="understanding-the-challenges">Understanding the Challenges</h2>

<h3 id="what-is-catastrophic-forgetting">What is Catastrophic Forgetting?</h3>

<p><strong>Catastrophic forgetting</strong> refers to the phenomenon where a neural network, upon learning new tasks, experiences a significant degradation in its performance on previously learned tasks. This occurs primarily in models trained using gradient-based optimization methods, where the updates to the model’s weights based on new data can overwrite the information encoded in those weights from prior tasks. As a result, the model effectively “forgets” what it has learned, leading to a decline in accuracy and reliability.</p>

<p>Mathematically, consider a neural network \( f \) parameterized by weights \( W \). When the model is trained on a sequence of tasks \( T_1, T_2, \ldots, T_n \), the objective is to minimize the loss function for each task:</p>

<p>\[
L(W) = \sum_{i=1}^{n} L_i(f(W, X_i), Y_i)
\]</p>

<p>where \( L_i \) is the loss function associated with task \( T_i \), \( X_i \) represents the input data, and \( Y_i \) denotes the corresponding output labels. As the model is exposed to new tasks, the optimization process can lead to weight adjustments that significantly alter the learned representations for earlier tasks, resulting in a performance drop on those tasks.</p>

<p><strong>Examples of Catastrophic Forgetting:</strong></p>
<ol>
  <li>
    <p><strong>Image Classification</strong>: A neural network trained to classify images of cats and dogs may perform well on this task. However, if it is subsequently trained on images of birds without any mechanism to retain the original knowledge, its accuracy on cat and dog images may plummet.</p>
  </li>
  <li>
    <p><strong>Natural Language Processing</strong>: A language model fine-tuned on a specific domain, such as medical texts, may lose its proficiency in general language understanding if it is later trained exclusively on technical jargon from a different domain.</p>
  </li>
</ol>

<p>These examples illustrate the detrimental impact of catastrophic forgetting, emphasizing the need for effective strategies to ensure that neural networks can retain previously acquired knowledge while adapting to new information.</p>

<h3 id="what-is-loss-of-plasticity">What is Loss of Plasticity?</h3>

<p><strong>Loss of plasticity</strong> refers to the diminishing ability of a neural network to learn and adapt to new tasks as it accumulates knowledge over time. While a model may initially exhibit high plasticity—characterized by rapid learning and adaptation—this ability can decline as the network becomes entrenched in its learned representations. This phenomenon is particularly concerning in continual learning scenarios, where the ability to assimilate new information is crucial.</p>

<p>Mathematically, plasticity can be conceptualized in terms of the model’s performance improvement over time when exposed to new data. Let \( p(t) \) represent the plasticity of the model at time \( t \) \( \), defined as the change in performance \( \Delta P \) relative to the learning rate \( \alpha \):</p>

<p>\[
p(t) = \frac{\Delta P(t)}{\alpha}
\]</p>

<p>As the model encounters more tasks, its plasticity may decrease, leading to a situation where the performance improvement \( \Delta P(t) \) becomes negligible despite ongoing training efforts. This decline in plasticity can manifest as slower convergence rates, reduced adaptability, and an overall inability to incorporate new knowledge effectively.</p>

<p><strong>Effects of Loss of Plasticity:</strong></p>
<ol>
  <li>
    <p><strong>Decreased Learning Rate</strong>: As a model learns multiple tasks, its learning rate may need to be adjusted downward to prevent instability, which can hinder efficient learning.</p>
  </li>
  <li>
    <p><strong>Inability to Generalize</strong>: A model that has lost plasticity may struggle to generalize knowledge from previously learned tasks to new ones, leading to a failure to apply relevant insights in novel contexts.</p>
  </li>
  <li>
    <p><strong>Increased Training Time</strong>: The time required to train the model on new tasks may increase significantly, as the network becomes less responsive to updates.</p>
  </li>
</ol>

<p>In summary, the loss of plasticity poses a substantial challenge to the effectiveness of continual learning systems, highlighting the necessity for methods that can sustain both adaptability and performance across diverse tasks.</p>

<h2 id="the-streaming-learning-context">The Streaming Learning Context</h2>

<h3 id="concept-of-streaming-learning">Concept of Streaming Learning</h3>

<p><strong>Streaming learning</strong> refers to a paradigm in machine learning where data is continuously generated and arrives in an incremental fashion, rather than being available in a static, pre-defined dataset. This approach mimics real-world scenarios where data is constantly evolving, such as in social media feeds, sensor data from IoT devices, or financial transactions. In streaming learning, the model must adapt to new information on-the-fly, making real-time predictions and updates without the luxury of retraining on the entire dataset.</p>

<p>Mathematically, let \( D(t) \) represent the dataset at time \( t \), which is incrementally updated as new data points \( x_i \) are received. The goal is to maintain a model \( f \) that optimally predicts the output \( y_i \) for each incoming data point:</p>

<p>\[
y_i = f(x_i; W(t))
\]</p>

<p>where \( W(t) \) denotes the model parameters at time \( t \). The model must continuously learn from the new data while minimizing a loss function \( L \):</p>

<p>\[
L(W(t)) = \sum_{i=1}^{n} L(f(x_i; W(t)), y_i)
\]</p>

<p>This requires the model to efficiently update its weights \( W(t) \) to accommodate the new information without losing performance on previously learned data.</p>

<h3 id="practical-implications-and-challenges">Practical Implications and Challenges</h3>

<p>The streaming learning context presents several practical implications and challenges that must be addressed to ensure effective model performance:</p>

<ol>
  <li>
    <p><strong>Limited Data Retention</strong>: In many streaming applications, it may be infeasible to store all incoming data due to memory constraints. This limitation necessitates the development of algorithms that can learn effectively from a limited number of samples, often referred to as <strong>online learning</strong>. Models must be designed to extract essential features and insights from each data point without relying on the entire dataset.</p>
  </li>
  <li>
    <p><strong>Non-Stationary Data Distribution</strong>: Data in streaming contexts is often non-stationary, meaning that the underlying distribution can change over time. This phenomenon, known as <strong>concept drift</strong>, can significantly impact model performance. For instance, a model trained on consumer behavior data may become obsolete as trends evolve. Mathematically, if the distribution of the data changes from $ P(t) $ to $ P(t + \Delta t) $, the model must adapt to maintain accuracy:</p>

    <p>\[
P(t) \neq P(t + \Delta t)
\]</p>

    <p>This requires mechanisms to detect drift and adjust the model accordingly, often through techniques such as adaptive learning rates or periodic retraining.</p>
  </li>
  <li>
    <p><strong>Real-Time Processing Requirements</strong>: In many applications, such as fraud detection or real-time recommendation systems, the model must provide predictions with minimal latency. This necessitates efficient algorithms capable of processing data quickly while still maintaining high accuracy. The trade-off between computation speed and model complexity must be carefully managed to meet real-time demands.</p>
  </li>
  <li>
    <p><strong>Catastrophic Forgetting</strong>: As discussed previously, the risk of catastrophic forgetting becomes particularly pronounced in streaming learning contexts. When new data is introduced, the model may overwrite previously learned information, leading to decreased performance on earlier tasks. Strategies such as memory management, weight regularization, or selective retraining are essential to mitigate this challenge.</p>
  </li>
  <li>
    <p><strong>Evaluation and Feedback Loops</strong>: In streaming learning, traditional evaluation methods that rely on fixed test sets may not be applicable. Continuous evaluation strategies must be implemented to assess model performance over time, often utilizing metrics that account for the dynamic nature of incoming data. Feedback loops are crucial for refining the model based on real-world performance, allowing for ongoing adjustments.</p>
  </li>
</ol>

<p>In summary, the streaming learning context embodies a complex and dynamic environment where models must continuously adapt to new data while coping with inherent challenges such as limited retention, non-stationarity, and real-time processing demands. Addressing these challenges is imperative for the successful deployment of machine learning systems in practical applications.</p>

<h2 id="introducing-upgd">Introducing UPGD</h2>

<h3 id="what-is-utility-based-perturbed-gradient-descent-upgd">What is Utility-based Perturbed Gradient Descent (UPGD)?</h3>

<p>Utility-based Perturbed Gradient Descent (UPGD) is a novel approach designed to address the challenges of catastrophic forgetting and loss of plasticity in continual learning scenarios. UPGD modifies the standard gradient descent algorithm by incorporating a utility-based mechanism that evaluates the importance of each weight in the neural network. This allows the model to make more informed updates that not only focus on minimizing the loss for new tasks but also protect previously learned knowledge.</p>

<p>The UPGD method operates by introducing perturbations to the weight updates. In traditional gradient descent, the update rule for the weights \(W \) is given by:</p>

<p>\[
W \leftarrow W - \alpha \nabla L(W)
\]</p>

<p>where \( \alpha \) is the learning rate and \( \nabla L(W) \) is the gradient of the loss function with respect to the weights. In UPGD, this update rule is modified to include a utility measure \( U \):</p>

<p>\[
W \leftarrow W - \alpha \nabla L(W) + \xi(1 - U)
\]</p>

<p>Here, \( \xi \) represents a noise term or perturbation, and \( U \) is a measure of the utility of the weights. This modification allows UPGD to not only optimize for the current task but also to adjust the updates based on the importance of each weight, effectively balancing the need to learn new information while preserving critical knowledge from past tasks.</p>

<h3 id="the-importance-of-utility-measures">The Importance of Utility Measures</h3>

<p>The utility measure \( U \) plays a pivotal role in the UPGD framework. It quantifies the significance of individual weights in contributing to the overall performance of the model. The utility of a weight \( w_{i,j} \) can be defined based on the change in loss that would occur if that weight were set to zero. Specifically, it can be formulated as:</p>

<p>\[
U_{i,j} = \frac{\partial L}{\partial w_{i,j}} \bigg|_{w_{i,j} = 0}
\]</p>

<p>This measure provides insight into how crucial a particular weight is for maintaining the model’s performance on previously learned tasks. Weights with high utility are deemed essential, while those with low utility can be more freely adjusted during training.</p>

<p>In the context of UPGD, the incorporation of utility measures allows the model to prioritize important weights while enabling adaptability to new tasks. By applying perturbations selectively based on the utility, UPGD can:</p>

<ol>
  <li>
    <p><strong>Preserve Critical Knowledge</strong>: Weights that are identified as crucial for earlier tasks are updated more conservatively, reducing the risk of catastrophic forgetting. This ensures that foundational knowledge remains intact even as the model learns new information.</p>
  </li>
  <li>
    <p><strong>Encourage Adaptability</strong>: Conversely, weights with lower utility can be adjusted more aggressively, allowing the model to adapt to new data and tasks without being overly constrained by past knowledge. This dynamic adjustment fosters a balance between stability and flexibility.</p>
  </li>
  <li>
    <p><strong>Facilitate Continuous Learning</strong>: By effectively managing the trade-off between preserving important weights and adapting to new information, UPGD enhances the model’s capacity for continual learning. This is particularly beneficial in streaming contexts, where data is constantly evolving and the model must remain responsive to changes.</p>
  </li>
</ol>

<p>In summary, UPGD represents a significant advancement in continual learning methodologies by integrating utility measures into the weight update process. This approach not only mitigates the challenges of catastrophic forgetting and loss of plasticity but also optimizes the model’s ability to learn and adapt over time.</p>

<h2 id="experimental-validation">Experimental Validation</h2>

<h3 id="overview-of-experiments">Overview of Experiments</h3>

<p>To evaluate the effectiveness of the Utility-based Perturbed Gradient Descent (UPGD) method, a series of experiments were conducted using well-established datasets commonly employed in the field of machine learning. The primary datasets utilized in these experiments included:</p>

<ol>
  <li>
    <p><strong>MNIST</strong>: The MNIST dataset consists of 70,000 grayscale images of handwritten digits (0-9), each measuring 28x28 pixels. It serves as a foundational benchmark for evaluating image classification algorithms and is particularly useful for assessing the capabilities of models in recognizing patterns and generalizing across different classes.</p>
  </li>
  <li>
    <p><strong>CIFAR-10</strong>: The CIFAR-10 dataset comprises 60,000 color images categorized into 10 distinct classes, with 6,000 images per class. The images are of size 32x32 pixels and present a more complex challenge than MNIST due to their diverse content and higher resolution. This dataset is widely used to test the robustness and adaptability of machine learning models in multi-class classification tasks.</p>
  </li>
</ol>

<h3 id="experimental-setup">Experimental Setup</h3>

<p>The experimental setup involved training neural network models on these datasets in a continual learning framework. The following steps were implemented:</p>

<ul>
  <li>
    <p><strong>Task Sequence</strong>: The experiments were structured to simulate a continual learning scenario, where the model was exposed to a sequence of tasks representing different classes from the datasets. For instance, in the case of CIFAR-10, the model would first learn to classify images from one subset of classes before being exposed to another subset.</p>
  </li>
  <li>
    <p><strong>Training Protocol</strong>: For each task, the model was trained using both the UPGD method and several baseline methods, including standard gradient descent and other existing continual learning approaches. The training process included periodic evaluations to measure the model’s performance on both current and previously learned tasks.</p>
  </li>
  <li>
    <p><strong>Performance Metrics</strong>: Key performance metrics were established to assess the model’s effectiveness, including accuracy on individual tasks, overall accuracy across all tasks, and measures of plasticity to evaluate the model’s ability to adapt to new information without forgetting previously learned knowledge.</p>
  </li>
</ul>

<h3 id="key-findings">Key Findings</h3>

<p>The results of the experiments demonstrated that UPGD significantly outperformed existing methods in several critical areas:</p>

<ol>
  <li>
    <p><strong>Enhanced Performance</strong>: UPGD exhibited superior accuracy compared to traditional gradient descent and other baseline methods. For instance, in the MNIST dataset, the model trained with UPGD achieved an accuracy of over 98% across all tasks, while baseline methods showed a decline in performance as new tasks were introduced.</p>
  </li>
  <li>
    <p><strong>Mitigation of Catastrophic Forgetting</strong>: One of the most notable findings was UPGD’s effectiveness in reducing catastrophic forgetting. The model retained high accuracy on previously learned tasks even as it adapted to new ones. For example, while a standard gradient descent approach demonstrated a drop in accuracy on earlier tasks by as much as 30% after learning new classes, UPGD maintained a much more stable performance, with a decline of only 5-10%.</p>
  </li>
  <li>
    <p><strong>Preservation of Plasticity</strong>: UPGD also excelled in maintaining plasticity, allowing the model to adapt to new tasks without significant loss of performance. The balance struck between preserving important weights and enabling flexibility resulted in a model that could learn efficiently from new data while retaining essential knowledge from past tasks. This was quantitatively assessed using plasticity metrics, where UPGD achieved higher scores than competing methods, indicating a greater capacity for learning new information.</p>
  </li>
  <li>
    <p><strong>Robustness Across Datasets</strong>: The effectiveness of UPGD was consistent across both MNIST and CIFAR-10 datasets, demonstrating its robustness and applicability to various tasks and complexities. The results indicate that UPGD is not only effective in simpler scenarios but also scales well to more challenging environments.</p>
  </li>
</ol>

<p>In conclusion, the experimental validation of UPGD highlights its significant advantages in continual learning contexts, particularly in mitigating catastrophic forgetting and preserving plasticity. These findings underscore the potential of UPGD as a transformative approach for developing resilient and adaptable machine learning systems capable of lifelong learning.</p>

<h2 id="implications-for-future-research">Implications for Future Research</h2>

<p>The introduction of Utility-based Perturbed Gradient Descent (UPGD) represents a significant advancement in the field of continual learning, offering promising avenues for future research and development. The implications of UPGD extend beyond immediate improvements in model performance; they also lay the groundwork for a deeper understanding of how neural networks can be designed to learn continuously and adaptively over time.</p>

<h3 id="potential-impact-on-future-developments-in-continual-learning">Potential Impact on Future Developments in Continual Learning</h3>

<ol>
  <li>
    <p><strong>Enhanced Model Architectures</strong>: The principles underlying UPGD can inspire the development of new neural network architectures that inherently incorporate utility measures. By embedding mechanisms for evaluating weight importance directly into the architecture, future models may achieve greater efficiency in learning and retention of knowledge across tasks.</p>
  </li>
  <li>
    <p><strong>Integration with Other Learning Paradigms</strong>: UPGD opens the door for integrating continual learning with other paradigms such as transfer learning and meta-learning. These integrative approaches could further enhance a model’s ability to generalize knowledge from one task to another, thereby improving adaptability and performance in diverse applications.</p>
  </li>
  <li>
    <p><strong>Real-World Applications</strong>: The ability of UPGD to mitigate catastrophic forgetting while maintaining plasticity makes it particularly suitable for real-world applications such as autonomous systems, robotics, and personalized recommendation systems. Future research can explore the deployment of UPGD in these domains, assessing its effectiveness in dynamic environments where data is constantly evolving.</p>
  </li>
  <li>
    <p><strong>Understanding Learning Dynamics</strong>: The utility measures used in UPGD provide a novel lens through which to study the dynamics of learning in neural networks. Future research could focus on understanding how different weight utilities evolve during training and the implications this has for model performance and knowledge retention.</p>
  </li>
</ol>

<h3 id="areas-for-further-exploration-and-improvement">Areas for Further Exploration and Improvement</h3>

<ol>
  <li>
    <p><strong>Optimizing Utility Measures</strong>: While the current formulation of utility measures is effective, further exploration into alternative definitions and methods for calculating utility could yield even better results. Investigating adaptive utility measures that evolve based on the learning context may enhance the model’s ability to prioritize weight updates dynamically.</p>
  </li>
  <li>
    <p><strong>Scalability to Large-Scale Problems</strong>: Future studies should examine the scalability of UPGD in large-scale and high-dimensional problems. Evaluating its performance on more complex datasets and tasks will provide insights into its robustness and adaptability in diverse scenarios.</p>
  </li>
  <li>
    <p><strong>Combining UPGD with Replay Mechanisms</strong>: Investigating the synergy between UPGD and replay-based methods could be fruitful. While UPGD effectively reduces forgetting, combining it with selective rehearsal of past tasks may further enhance knowledge retention and performance on a broader range of tasks.</p>
  </li>
  <li>
    <p><strong>Exploring Different Learning Rates</strong>: The impact of varying learning rates on the performance of UPGD could be a valuable area of research. Understanding how different learning rates influence the balance between stability and adaptability will help refine training protocols and improve overall model efficacy.</p>
  </li>
  <li>
    <p><strong>Empirical Validation in Diverse Domains</strong>: Conducting empirical studies across various domains, such as natural language processing, healthcare, and finance, will provide a comprehensive understanding of UPGD’s applicability and effectiveness. Such research can help tailor UPGD to address specific challenges inherent to different fields.</p>
  </li>
</ol>

<p>In conclusion, the introduction of UPGD not only enhances the capabilities of continual learning models but also paves the way for future advancements in the field. By exploring the outlined areas for further research, the machine learning community can continue to develop more robust, adaptable, and efficient systems that are capable of lifelong learning and continuous adaptation to new challenges.</p>

<h2 id="conclusion">Conclusion</h2>

<p>In summary, the challenges of <strong>catastrophic forgetting</strong> and <strong>loss of plasticity</strong> are critical hurdles in the advancement of continual learning methodologies. Catastrophic forgetting leads to a significant decline in a model’s performance on previously learned tasks when exposed to new information, while loss of plasticity restricts the model’s ability to adapt and learn effectively over time. Addressing these issues is essential for developing robust and resilient machine learning systems that can operate in dynamic environments, where data is continuously generated and evolves.</p>

<p>The introduction of <strong>Utility-based Perturbed Gradient Descent (UPGD)</strong> represents a promising solution to these challenges. By integrating utility measures into the weight update process, UPGD effectively balances the need to learn new tasks while preserving critical knowledge from past experiences. The experimental validation of UPGD demonstrates its superiority over traditional methods, highlighting its potential to maintain both accuracy and plasticity in continual learning scenarios.</p>

<p>We encourage readers to explore the paper titled <strong>“Addressing Loss of Plasticity and Catastrophic Forgetting in Continual Learning,”</strong> which details the UPGD methodology and its experimental outcomes. The findings presented in this work not only contribute to the understanding of continual learning but also offer a foundation for future research aimed at enhancing the adaptability and efficiency of machine learning models. By delving into this paper, readers will gain valuable insights into the innovative approaches that can drive the field of artificial intelligence forward, paving the way for systems that learn and evolve as seamlessly as humans do.</p>]]></content><author><name>Lhuqita Fazry</name><email>lhuqita.fazry@ui.ac.id</email></author><category term="research" /><category term="machine learning" /><category term="continual learning" /><summary type="html"><![CDATA[In the realm of artificial intelligence (AI) and machine learning, continual learning—also known as lifelong learning—refers to the ability of a model to learn from a continuous stream of data over time, adapting to new information while retaining previously acquired knowledge. This contrasts with traditional machine learning paradigms, which often involve training models on static datasets, leading to a fixed understanding of the problem domain. Continual learning aims to emulate human-like learning processes, where individuals build upon past experiences and knowledge as they encounter new situations.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/post-thumbnails/2025-05-13-advancing-continual-learning-tackling-catastrophic-forgetting-and-loss-of-plasticity-with-upgd.jpg" /><media:content medium="image" url="http://localhost:4000/assets/post-thumbnails/2025-05-13-advancing-continual-learning-tackling-catastrophic-forgetting-and-loss-of-plasticity-with-upgd.jpg" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>